import torch
import torch.nn.utils.prune as prune
import torch.quantization
import time
from torch.utils.data import DataLoader
from torchvision import transforms
import os
import sys

current_dir = os.getcwd()  
sys.path.append(current_dir)

from AI.Model.CNN import train


# âœ… Pruning ì ìš© í•¨ìˆ˜ (CPUë¡œ ì´ë™ ì¶”ê°€)
def apply_pruning(model, amount=0.5):
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=amount)
            prune.remove(module, 'weight')  # ì‹¤ì œë¡œ pruning ì ìš©
    model.to("cpu")  # âœ… Pruning í›„ CPUë¡œ ì´ë™
    return model


# âœ… Quantization ì ìš© í•¨ìˆ˜
def apply_quantization(model):
    model.eval()  # âœ… Evaluation mode ì„¤ì •
    quantized_model = torch.quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8  # FC ë ˆì´ì–´ë¥¼ 8ë¹„íŠ¸ë¡œ ë³€í™˜
    )
    return quantized_model


# âœ… ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ (CPU & GPU ì§€ì›)
def evaluate_model(model, test_loader, device):
    model.to(device)
    model.eval()
    total_time = 0
    total_images = 0

    with torch.no_grad():
        for images, _ in test_loader:
            images = images.to(device)
            batch_size = images.shape[0]  # í˜„ì¬ ë°°ì¹˜ í¬ê¸°
            total_images += batch_size

            start_time = time.time()
            _ = model(images)
            end_time = time.time()

            total_time += (end_time - start_time)

    avg_batch_time = (total_time / len(test_loader)) * 1000  # ms ë‹¨ìœ„ ë³€í™˜
    avg_sample_time = (total_time / total_images) * 1000  # ms ë‹¨ìœ„ ë³€í™˜ (1ì¥ ì²˜ë¦¬ ì‹œê°„)
    return avg_batch_time, avg_sample_time


# âœ… ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
def main():
    # ì¥ì¹˜ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using {device} device")

    transform = transforms.Compose([
        transforms.Resize((16, 16)),
        transforms.ToTensor()
    ])

    # ë°ì´í„°ì…‹ ë¡œë“œ
    normal_dataset = train.PacketImageDataset(
        './Data/save/save_packet_to_byte_16/front_image',
        transform=transform,
        is_flat_structure=True
    )

    attack_dataset = train.PacketImageDataset(
        './Data/attack/attack_to_byte_16',
        transform=transform,
        is_flat_structure=False
    )

    generator = torch.Generator().manual_seed(42)

    normal_train_size = int(0.8 * len(normal_dataset))
    normal_test_size = len(normal_dataset) - normal_train_size
    normal_train_dataset, normal_test_dataset = torch.utils.data.random_split(
        normal_dataset, [normal_train_size, normal_test_size],
        generator=generator
    )

    attack_train_size = int(0.8 * len(attack_dataset))
    attack_test_size = len(attack_dataset) - attack_train_size
    attack_train_dataset, attack_test_dataset = torch.utils.data.random_split(
        attack_dataset, [attack_train_size, attack_test_size],
        generator=generator
    )

    test_normal_loader = DataLoader(normal_test_dataset, batch_size=256, shuffle=False)
    test_attack_loader = DataLoader(attack_test_dataset, batch_size=256, shuffle=False)

    # âœ… ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    model = train.SimplePacketCNN().to(device)
    model.load_state_dict(torch.load('./AI/Model/CNN/packet_classifier_front_16.pth'))
    model.eval()

    # âœ… ê¸°ë³¸ ëª¨ë¸ í‰ê°€ (GPU)
    base_batch_time_gpu, base_sample_time_gpu = evaluate_model(model, test_normal_loader, device)
    print(f"âœ… ê¸°ë³¸ ëª¨ë¸ í‰ê·  ë°°ì¹˜ ì¶”ë¡  ì‹œê°„ (GPU): {base_batch_time_gpu:.4f} ms")
    print(f"âœ… ê¸°ë³¸ ëª¨ë¸ í‰ê·  1ì¥ ì²˜ë¦¬ ì‹œê°„ (GPU): {base_sample_time_gpu:.4f} ms")

    # âœ… ê¸°ë³¸ ëª¨ë¸ í‰ê°€ (CPU)
    base_batch_time_cpu, base_sample_time_cpu = evaluate_model(model, test_normal_loader, "cpu")
    print(f"âœ… ê¸°ë³¸ ëª¨ë¸ í‰ê·  ë°°ì¹˜ ì¶”ë¡  ì‹œê°„ (CPU): {base_batch_time_cpu:.4f} ms")
    print(f"âœ… ê¸°ë³¸ ëª¨ë¸ í‰ê·  1ì¥ ì²˜ë¦¬ ì‹œê°„ (CPU): {base_sample_time_cpu:.4f} ms")

    # âœ… Pruning ì ìš© (CPUë¡œ ì´ë™ë¨)
    pruned_model = apply_pruning(model)
    pruned_batch_time, pruned_sample_time = evaluate_model(pruned_model, test_normal_loader, "cpu")
    print(f"âœ… Pruned ëª¨ë¸ í‰ê·  ë°°ì¹˜ ì¶”ë¡  ì‹œê°„ (CPU): {pruned_batch_time:.4f} ms")
    print(f"âœ… Pruned ëª¨ë¸ í‰ê·  1ì¥ ì²˜ë¦¬ ì‹œê°„ (CPU): {pruned_sample_time:.4f} ms")

    # âœ… Quantization ì ìš© (CPUì—ì„œ ì‹¤í–‰)
    quantized_model = apply_quantization(pruned_model)
    quantized_batch_time, quantized_sample_time = evaluate_model(quantized_model, test_normal_loader, "cpu")
    print(f"âœ… Pruned + Quantized ëª¨ë¸ í‰ê·  ë°°ì¹˜ ì¶”ë¡  ì‹œê°„ (CPU): {quantized_batch_time:.4f} ms")
    print(f"âœ… Pruned + Quantized ëª¨ë¸ í‰ê·  1ì¥ ì²˜ë¦¬ ì‹œê°„ (CPU): {quantized_sample_time:.4f} ms")

    # âœ… ìµœì í™” ê²°ê³¼ ë¹„êµ
    print("\nğŸ”¹ ìµœì í™” ê²°ê³¼:")
    print(f"ê¸°ë³¸ ëª¨ë¸ (GPU) : ë°°ì¹˜ {base_batch_time_gpu:.4f} ms | 1ì¥ {base_sample_time_gpu:.4f} ms")
    print(f"ê¸°ë³¸ ëª¨ë¸ (CPU) : ë°°ì¹˜ {base_batch_time_cpu:.4f} ms | 1ì¥ {base_sample_time_cpu:.4f} ms")
    print(f"Pruned ëª¨ë¸  (CPU) : ë°°ì¹˜ {pruned_batch_time:.4f} ms | 1ì¥ {pruned_sample_time:.4f} ms")
    print(f"Pruned + Quantized ëª¨ë¸ (CPU) : ë°°ì¹˜ {quantized_batch_time:.4f} ms | 1ì¥ {quantized_sample_time:.4f} ms")


if __name__ == '__main__':
    main()
